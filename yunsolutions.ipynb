{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Hwr4M2mCfyNT",
        "outputId": "346c356c-5237-4316-9ec2-87bbdc8b1814"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8b097fbb984b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "# Read in the Excel file (attached - corpo_announcement) using the pandas' library.\n",
        "# Extract the relevant columns(like, HEADLINE, NEWSSUB, MORE) containing the earnings call dates, URLs, and any other relevant information.\n",
        "# (If required info is not found in the text then read the pdf which is present in the \"SOURCE\" column and extract info from that.)\n",
        "# Use regular expressions or other methods to identify the links to the earnings call transcripts and audio files within the URL column.\n",
        "# Output a new Excel file containing the relevant data, along with columns containing the links to the transcripts and audio files.\n",
        "# Note that you may need to use web scraping techniques to actually download the transcripts and audio files once you have identified their URLs. \n",
        "# Be sure to properly handle any errors or exceptions that may occur during the data extraction process.\n",
        "\n",
        "#1. extract data\n",
        "#2. filter relevant data (headline, newssub, more, source,·∏çttm, newsdt, nsurl,) \n",
        "#3. if not found, extract from the pdf\n",
        "\n",
        "sheet = pd.read_excel('corpo_announcements.xlsx', sheet_name=\"Sheet1\")\n",
        "columns = ['HEADLINE', 'NEWSSUB', 'DT_TM', 'MORE', 'NEWS_DT', 'NSURL', 'XML_NAME', 'SOURCE']\n",
        "sheet = sheet[columns]\n",
        "\n",
        "audiolinks = []\n",
        "\n",
        "for url in sheet['NSURL']:\n",
        "    print(url)\n",
        "    uri = requests.get(url)\n",
        "    print(uri)\n",
        "    soup = BeautifulSoup(uri.text, \"html.parser\")\n",
        "    corp = soup.find_all('a', attrs={'class':\"single-item\"})\n",
        "    for cor in corp:\n",
        "        if corp.content == \" Corp Announcements \":\n",
        "            tul = corp['href']\n",
        "            \n",
        "    uri2 = requests.get(tul)\n",
        "    soup = BeautifulSoup(uri2.text, \"html-parser\")\n",
        "    tables = soup.find('table')\n",
        "    tabler = tables.find('table')\n",
        "    tables = tabler.find('table')\n",
        "    tabler = tables.find_all('table', attrs={'class': \"ng-scope\"})\n",
        "    for table in tabler:\n",
        "        aku = table.find('a')\n",
        "        if aku is not None:\n",
        "            tri = str(aku.content)\n",
        "            tr = \"Earnings\"\n",
        "            if tr in tri:\n",
        "                aki = table.find_next('a')\n",
        "                if aki is not None:\n",
        "                    urli = aki['href']\n",
        "                    urli = str(urli)\n",
        "                    urli = \"https://www.bseindia.com\" + urli\n",
        "                    response = requests.get(urli)\n",
        "                    with open(\"doc.pdf\", \"wb\") as file:\n",
        "                        file.write(response.content)\n",
        "                    pdfile = open('doc.pdf', 'rb')\n",
        "                    pdfreader = PyPDF2.PdfFileReader(pdfile)\n",
        "                    \n",
        "                    for page in range(pdfreader.getNumPages()):\n",
        "                        page_obj = pdfreader.getPage(page)\n",
        "                        page_text = page_obj.extract_text()\n",
        "                        ulu = re.findall(\"(?P<url>https?://[^\\s]+)\", page_text)\n",
        "                        audiolinks.append(ulu)\n",
        "\n",
        "\n",
        "sheet['AUDIO_LINK'] = audiolinks\n",
        "\n",
        "sheet.to_excel('corpo_announcements_extracted.xlsx', index = False)\n",
        " \n",
        "                    \n",
        "                    \n",
        "        "
      ]
    }
  ]
}